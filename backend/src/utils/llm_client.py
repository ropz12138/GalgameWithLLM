"""
LLMå®¢æˆ·ç«¯å·¥å…·ç±» - ç»Ÿä¸€ç®¡ç†LLMè°ƒç”¨
"""
import os
import json
import logging
from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

logger = logging.getLogger(__name__)


class LLMClient:
    """LLMå®¢æˆ·ç«¯ç±»"""
    
    def __init__(self):
        self._llm_instance = None
        self._config = None
        self._load_config()
        self._initialize_llm()
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            config_path = os.path.join(os.path.dirname(__file__), '../../config/config.json')
            with open(config_path, 'r', encoding='utf-8') as f:
                self._config = json.load(f)
            logger.info("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self._config = {}
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–LLMå®ä¾‹"""
        try:
            if not self._config or 'llm' not in self._config:
                logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°LLMé…ç½®")
                return
            
            # é»˜è®¤ä½¿ç”¨geminié…ç½®
            llm_configs = self._config['llm']
            
            # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©å¯ç”¨çš„LLM
            for provider in ['gemini', 'qwen', 'doubao', 'xai']:
                if provider in llm_configs:
                    config = llm_configs[provider]
                    logger.info(f"ğŸ”§ ä½¿ç”¨LLMé…ç½®: {provider}")
                    
                    self._llm_instance = ChatOpenAI(
                        api_key=config['api_key'],
                        base_url=config['url'],
                        model=config['model'],
                        temperature=0.7,
                        max_tokens=2000,
                        timeout=30
                    )
                    
                    logger.info(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ - æä¾›å•†: {provider}, æ¨¡å‹: {config['model']}")
                    return
            
            logger.error("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„LLMé…ç½®")
            
        except Exception as e:
            logger.error(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self._llm_instance = None
    
    def get_llm_instance(self):
        """è·å–LLMå®ä¾‹"""
        if self._llm_instance is None:
            self._initialize_llm()
        return self._llm_instance
    
    async def chat_completion(self, prompt: str, system_message: Optional[str] = None) -> str:
        """
        è°ƒç”¨LLMè¿›è¡Œå¯¹è¯å®Œæˆ
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_message: ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            LLMå“åº”
        """
        try:
            llm = self.get_llm_instance()
            if llm is None:
                return "æŠ±æ­‰ï¼ŒLLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
            
            messages = []
            if system_message:
                messages.append(SystemMessage(content=system_message))
            messages.append(HumanMessage(content=prompt))
            
            response = await llm.ainvoke(messages)
            return response.content
            
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼ŒLLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
    
    def get_current_model_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯"""
        if not self._config or 'llm' not in self._config:
            return {}
        
        llm_configs = self._config['llm']
        for provider in ['gemini', 'qwen', 'doubao', 'xai']:
            if provider in llm_configs:
                config = llm_configs[provider]
                return {
                    'provider': provider,
                    'model': config['model'],
                    'url': config['url']
                }
        return {}

    def is_available(self) -> bool:
        """æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨"""
        return self._llm_instance is not None 