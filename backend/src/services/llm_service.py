"""
LLMæœåŠ¡ - å¤„ç†å¤§è¯­è¨€æ¨¡å‹ç›¸å…³é€»è¾‘
"""
import os
import json
from typing import Dict, Any, Optional
from langchain_openai import ChatOpenAI


class LLMService:
    """LLMæœåŠ¡ç±»"""
    
    def __init__(self):
        self._llm_instance = None
        self._config = None
    
    def load_config(self) -> Optional[Dict[str, Any]]:
        """ä»config.jsonåŠ è½½é…ç½®"""
        if self._config is not None:
            return self._config
        
        config_path = os.path.join(os.path.dirname(__file__), '..', '..', 'config', 'config.json')
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self._config = json.load(f)
                return self._config
        except FileNotFoundError:
            print(f"âš ï¸  é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            return None

    def get_llm_config(self, model_name: str = "gemini") -> Optional[Dict[str, str]]:
        """
        è·å–LLMé…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§° (gemini, doubao, xai, qwen)
            
        Returns:
            LLMé…ç½®
        """
        config = self.load_config()
        if not config:
            # ä½¿ç”¨é»˜è®¤é…ç½®ä½œä¸ºåå¤‡
            return {
                "url": "https://generativelanguage.googleapis.com/v1beta/openai/",
                "model": "gemini-2.5-flash-preview-05-20",
                "api_key": "AIzaSyDTjUqOBRlJpY9H55Bb5dzOQ8JyhgrirzE"
            }
        
        # è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®
        llm_config = config.get("llm", {}).get(model_name, {})
        if not llm_config:
            print(f"âš ï¸  æœªæ‰¾åˆ°{model_name}é…ç½®ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„LLMé…ç½®")
            # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„LLMé…ç½®
            llm_configs = config.get("llm", {})
            if llm_configs:
                first_config = list(llm_configs.values())[0]
                llm_config = first_config
            else:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•LLMé…ç½®")
                return None
        
        return llm_config

    def get_llm_instance(self, model_name: str = "gemini") -> ChatOpenAI:
        """
        è·å–LLMå®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            LLMå®ä¾‹
        """
        if self._llm_instance is None:
            llm_config = self.get_llm_config(model_name)
            if not llm_config:
                raise ValueError("æ— æ³•è·å–LLMé…ç½®")
            
            print(f"ğŸ”§ ä½¿ç”¨LLMé…ç½®: {llm_config.get('model', 'unknown')}")
            
            self._llm_instance = ChatOpenAI(
                model_name=llm_config.get("model", "gemini-2.5-flash-preview-05-20"),
                openai_api_key=llm_config.get("api_key"),
                openai_api_base=llm_config.get("url"),
                temperature=0.7,
            )
        
        return self._llm_instance
    
    def clean_llm_response(self, response) -> str:
        """
        æ¸…ç†LLMå“åº”ï¼Œç¡®ä¿åªè¿”å›contentå†…å®¹
        
        Args:
            response: LLMå“åº”
            
        Returns:
            æ¸…ç†åçš„å“åº”å†…å®¹
        """
        if response is None:
            return ""
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        if isinstance(response, str):
            return response
        
        # å¦‚æœæœ‰contentå±æ€§ï¼Œè¿”å›content
        if hasattr(response, 'content'):
            return response.content
        
        # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å–content
        if isinstance(response, dict):
            return response.get('content', str(response))
        
        # å…¶ä»–æƒ…å†µè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(response)

    async def invoke_llm(self, prompt: str, model_name: str = "gemini") -> str:
        """
        è°ƒç”¨LLM
        
        Args:
            prompt: æç¤ºè¯
            model_name: æ¨¡å‹åç§°
            
        Returns:
            LLMå“åº”
        """
        try:
            llm = self.get_llm_instance(model_name)
            response = await llm.ainvoke(prompt)
            return self.clean_llm_response(response)
        except Exception as e:
            print(f"è°ƒç”¨LLMå¤±è´¥: {e}")
            return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
    
    def reset_llm_instance(self):
        """é‡ç½®LLMå®ä¾‹"""
        self._llm_instance = None
    
    def get_available_models(self) -> Dict[str, Dict[str, str]]:
        """
        è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            å¯ç”¨æ¨¡å‹é…ç½®
        """
        config = self.load_config()
        if not config:
            return {}
        
        return config.get("llm", {})
    
    def test_llm_connection(self, model_name: str = "gemini") -> Dict[str, Any]:
        """
        æµ‹è¯•LLMè¿æ¥
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        try:
            llm = self.get_llm_instance(model_name)
            response = llm.invoke("ä½ å¥½ï¼Œè¯·ç®€å•å›å¤'è¿æ¥æˆåŠŸ'ã€‚")
            content = self.clean_llm_response(response)
            
            return {
                "success": True,
                "model": model_name,
                "response": content,
                "message": "è¿æ¥æˆåŠŸ"
            }
        except Exception as e:
            return {
                "success": False,
                "model": model_name,
                "error": str(e),
                "message": "è¿æ¥å¤±è´¥"
            }

if __name__ == "__main__":
    # Test the LLM instance
    try:
        llm_service = LLMService()
        response = llm_service.get_llm_instance().invoke("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
        print("LLM Response:", response)
        print("Cleaned Response:", llm_service.clean_llm_response(response))
    except Exception as e:
        print(f"Error invoking LLM: {e}")

